---
title: "Guidelines for reliable and reproducible functional enrichment analysis"
author: "Kaumadi Wijesooriya, Sameer A Jadaan, Tanuveer Kaur, Kaushalya L Perera, Mark Ziemann"
date: "`r Sys.Date()`"
output:
  rmdformats::robobook:
  html_document:
    toc: true
    toc_float: true
    fig_width: 7
    fig_height: 5
theme: cosmo
---

Source: https://github.com/markziemann/SurveyEnrichmentMethods

Target Journal: Briefings in Bioinformatics (JIF=9.0)

Kaumadi Wijesooriya<sup>1</sup>, Sameer A Jadaan<sup>2</sup>, Tanuveer Kaur<sup>1</sup>, Kaushalya L Perera<sup>1</sup>, Mark Ziemann<sup>1</sup>

**Affiliations**

1. Deakin University, Geelong, Australia, School of Life and Environmental Sciences.

2. College of Health and Medical Technology, Middle Technical University, Baghdad, Iraq.

## Abstract

Gene set enrichment tests (a.k.a. functional enrichment analysis) are among the most frequently used methods in computational biology.
Despite this popularity, there are concerns that these methods are being applied incorrectly and the results of some peer-reviewed publications are unreliable.
These methodological problems include the use of inappropriate background gene lists, lack of false discovery rate correction and combining up-and downregulated gene sets before enrichment analysis.
An example analysis of public RNA-seq reveals that these methodological errors alter enrichment results dramatically.
To ascertain the frequency of these errors in the literature, we performed a screen of 1363 open access research articles describing functional enrichment results.
We find that 95% of analyses using over-representation tests did not implement an appropriate reference gene list or did not describe this in the methods.
Failure to perform p-value correction for multiple tests was identified in 53% of analyses.
Many studies lacked detail in the methods section about the tools and gene sets used.
Only 10% of studies achieved an overall satisfactory standard, which highlights the poor state of functional enrichment rigour and reporting in the contemporary literature.
We provide a set of minimum standards that should act as a checklist for researchers and peer-reviewers.

## Introduction

Since the turn of the millennium, high throughput "omics" techniques like microarrays and next generation sequencing have brought with them a deluge of data.
These experiments involve the measurement of thousands of genes simultaneously, and can identify hundreds or even thousands of significant associations with developmental stages or diseases.
Interpreting such data is extraordinarily challenging, as the sheer number of associations can be difficult to investigate in a gene-by-gene manner.
Instead, many tools have been developed in an effort to summarise gene profiles into simplified functional categories.
These functional categories typically represent signaling or biochemical pathways, curated from information present in the literature, hence the name functional enrichment.

Widely used functional enrichment tools can be classified into two main categories; (i) over-representation analysis (ORA) and (ii) functional class scoring (FCS), and the most common application is in differential gene expression analysis.
In ORA, differentially expressed genes (DEGs) meeting a significance and/or fold change threshold are queried against curated pathways (gene sets).
A statistical test is performed to ascertain whether the number of DEGs belonging to a particular pathway (gene set) is higher than that expected by random chance.
ORA tools can be stand alone software packages or web services, and they use one or more statistical tests (eg: Fisher’s Exact test, hypergeometric test, binomial test) (Draghici et al, 2003; Hosack et al, 2003).
FCS tools involve giving each detected gene a differential expression score and then evaluating whether the scores are more positive or negative than expected by chance for each set of genes.
The popular Gene Set Enrichment Analysis (GSEA) tool uses permutation approaches to establish whether a gene set is significantly associated with higher or lower scores, either by permuting sample labels or by permuting genes in the differential expression profile (Subramanian et al, 2005).
From a user's perspective, ORA is easier to conduct because it is as simple as pasting a list of gene names into a text box on a website.
On the other hand FCS tools are more difficult to use but are more sensitive at detecting subtle associations (Kaspi and Ziemann 2020; Maleki et al, 2020; Xie et al, 2021).

Although these are powerful tools to summarise complex genomics data, there are concerns that they are not being correctly used.
Timmons et al (2015) highlight instances where a failure to account for sampling bias in ORA leads to biased results and invalidated conclusions in published work.
Many times as peer reviewers, we have had to request authors include details about what reference or background gene list was used to account for sampling bias.
In other cases we have requested authors to correct their p-values to account for the possibility of false positives when performing hundreds to thousands of gene set tests in parallel.
The purpose of this work is to demonstrate the effect of misusing enrichment tools and determine how frequent methodological issues in functional enrichment are, specifically 
(i) lack of a background gene set, 
(ii) lack of adjustment for multiple comparisons and 
(ii) lack of essential methodological details.
By performing a screen of published articles we will identify the most frequent error modes, and use this to inform us to develop a set of minimum standards for functional enrichment analysis.

## Methods

### An example gene set analysis

To demonstrate the effect of misusing gene set analysis, a publicly available RNA-seq dataset (accession SRP128998) was downloaded from DEE2 on 13th August 2021 (Ziemann et al, 2019).
This data consists of immortalised human hepatocytes cultured in standard or high glucose, first described by Felisbino et al (2021).
Transcript level counts were aggregated to genes using the getDEE2 R package v1.2.0. 
Next, genes with an average of 10 reads per sample were discarded.
Differential expression statistical analysis was conducted with DESeq2 v1.32.0 (Love et al, 2014) to identify genes altered by high glucose exposure.
For gene set analysis, Reactome gene sets (Jassal et al, 2020) were downloaded (URL: https://reactome.org/download/current/ReactomePathways.gmt.zip , accessed 13th August 2021).
FCS was performed using the mitch R package v1.4.0 with default settings, which uses a rank-ANOVA statistical test (Kaspi and Ziemann 2020).
ORA analysis was performed using the clusterProfiler R package (v4.0.2) enricher function that implements a hypergeometric test (Yu et al, 2012).
For ORA, two types of background gene sets were used (i) all detected genes, or (ii) all genes in the annotation set.
For genes and gene sets, a false discovery rate adjusted p-value (FDR) of 0.05 was considered significant.
Analyses were conducted in R version 4.1.0.

### Survey of enrichment analysis 

We have collated a list of 2941 articles in PubMed Central published in 2019 that have keywords "enrichment analysis", "pathway analysis" or "ontology analysis".
We sampled 200 of these articles and collected the following information from the article, searching the methods sections and other parts of the article including the supplement.

* Journal name

* Type of omics data

* Gene set library used, and whether a version was reported

* Statistical test used

* Whether p-values were corrected for multiple comparisons

* Software package used, and whether a version was reported

* Whether a background gene set was used

* Code availability

* Whether gene profile was provided in the supplement

* Whether assumptions was correctly applied

We excluded articles describing novel enrichment analysis techniques/tools, review articles and conference abstracts.
Some articles presented the results of >1 enrichment analysis, so additional rows were added to accommodate them. 
These data were entered into a Google Spreadsheet by a team of five researchers.
These articles were cross checked by another team member and any discrepancies were resolved.
For quality control, 200 randomly selected assessed articles were cross-checked by two senior team members to ensure consistency across the five team members.
Cleaned data were then loaded into R v4.1 for downstream analysis.

We rated each analysis with a simple approach that deducted points for missing methodological details and awarding points for including extra information (Table 1). 

| 1 point deducted | 1 point awarded |
| --- | --- |
| Gene set library origin not stated | Code made available |
| Gene set library version not stated | Gene lists provided |
| Stat test not stated | |
| No stat test conducted | |
| No FDR correction conducted | |
| App used not stated | |
| App version not stated | |
| Background list not defined | |
| Inappropriate background list used | |

Scimago Journal Ranks were downloaded from the Scimago website (https://www.scimagojr.com/journalrank.php) and used to rank journals by their citation metrics.
Using NCBI’s Eutils API, we collected the number of citations each article accrued since publication.
We used Spearman and Pearson correlation tests to determine any association with the analysis scores we generated.
All data analyses were conducted in R v4.1 and the analysis scripts are available at GitHub (https://github.com/markziemann/SurveyEnrichmentMethods).

## Results

### An example of gene set analysis misuse

In order to demonstrate the effect of gene set test misuse, we used an example RNA-seq dataset examining the effect of high glucose exposure on hepatocytes.
Out of 39,297 genes in the annotation set, 15,635 were above the detection threshold (>10 reads per sample on average).
Statistical analysis revealed 3,472 differentially expressed genes with 1,560 up-regulated and 1,912 down-regulated (FDR<0.05) due to glucose exposure.
FCS revealed 95 up and 316 downregulated pathways (FDR<0.05).
ORA with a background gene list consisting of detected genes revealed 55 upregulated and 238 downregulated gene sets (FDR<0.05).
There was a strong overlap between FCS and ORA results with a Jaccard statistic of 0.66 (Figure 1A).
We then performed ORA using the whole genome gene list (indicated as ORA\*), which resulted in 147 up and 530 down regulated gene sets (FDR<0.05),
The overlap of ORA with ORA\* was relatively smaller, with a Jaccard statistic of 0.41 (Figure 1B).
Interestingly, 26 gene sets were simultaneously up and downregulated with this approach.
Then we performed ORA analysis after combining up and downregulated genes as sometimes observed in the literature (indicated as ORA comb) with the standard background and again with a whole genome background (indicated as ORA\* comb).
ORA comb revealed only 22 differentially regulated gene sets with a Jaccard similarity to ORA of only 0.04 (Figure 1C). 
ORA\* comb yielded 64 differentially regulated gene sets with a Jaccard similarity to ORA of only 0.08.
These results demonstrate that commonly observed practices of combining the up and downregulated gene lists and using the whole genome background gene list severely distort ORA results.

![Figure 1A. Comparison of FCS and ORA methods.](images/fcs_ora1.png "FCS vs ORA")

![Figure 1B. ORA with standard and whole genome background.](images/orabg1.png "ORA with different background sets")

![Figure 1C. Effect of combining up and downregulated genes on ORA results.](images/oracomb1.png "ORA combining up and downregulated genes")


### A screen of functional enrichment analyses in open access journals

A search of PubMed Central showed 2941 articles published in 2019 with the keywords "enrichment analysis", "pathway analysis" or "ontology analysis".
From these, 1500 articles were screened for methodological errors; data supplied in Supplementary Table 1.
We excluded 133 articles from the screen because they did not present any enrichment analysis. 
Those excluded articles included articles describing novel enrichment analysis techniques or tools, review articles or conference abstracts.
As some articles included more than one enrichment analysis, the final dataset included 1624 analyses from 1363 articles (Supplementary Table 1).
A flow diagram of the survey is provided in Figure 2.

```{r,diagram1,echo=FALSE}

library(DiagrammeR)
grViz("digraph flowchart {
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']
      tab4 [label = '@@4']
      tab5 [label = '@@5']
      tab6 [label = '@@6']
      tab7 [label = '@@7']
      tab8 [label = '@@8']
      
      # edge definitions with the node IDs
      tab1 -> tab2  -> tab4 -> tab5 -> tab6 -> tab7;
      tab2 -> tab3 ;
      tab6 -> tab8 ;
      }
      [1]: 'PMC keyword search (2019): 2941 results'
      [2]: '200 PMC articles selected'
      [3]: '14 articles excluded'
      [4]: '235 analyses screened and cross-checked'
      [5]: 'data entry to spreadsheet'
      [6]: 'data analysis in R'
      [7]: 'Summary charts'
      [8]: 'Integration with journal and article metrics'
")

```
*Figure 2. A summary of the survey of functional enrichment analyses.*

There were articles from 100 journals in the sample, with *PLoS One*, *Scientific Reports*, and *PeerJ* being the biggest contributors (Figure S1A).
There were 18 different omics types, with gene expression array and RNA-seq being the most popular (Figure S1B).
There were 31 different species under study, but human was the most common with 157 analyses (Figure S1C).
We recorded the use of 26 different gene set libraries, with GO and KEGG being the most frequently used (Figure 3A).
There were 14 analyses where the gene set libraries used were not defined in the article.
Only 18 analyses reported the version of the gene set library used (Figure 3B).
There were 12 different statistical tests used, and the most common reported tests were Fisher's Exact, GSEA and hypergeometric tests; but the statistical test used was not reported for the majority of analyses (Figure 3C).
Out of the 225 analyses that performed a statistical test, only 119 (53%) described correction of p-values for multiple testing (Figure 3D).
There were 50 different tools used to perform enrichment analysis, with DAVID and GSEA being the most common; while 15 analyses (6.4%) did not state what tool was used (Figure 3E).
The version of the software used was provided in only 68 of 235 analyses (29%) (Figure 3F).

For analyses using ORA methods, we studied what background gene set was used (Figure 3G). 
This revealed that in most cases, the background list was not stated or it was clear from the article methods section that no background list was used.
In a few cases, background list was mentioned but was inappropriate, for example using a whole genome background for an assay like RNA-seq.
In only 8/197 of cases (4.1%), the appropriate background list was described in the article.

Of the 47 analyses which used a scripted computer language, only 3 provided links to the code used for enrichment analysis (6.4%) (Figure 3H).
For 93 of 235 analyses (40%), the corresponding gene lists/profiles were provided either in the supplement or in the article itself (Figure 3I).

Next, we quantified the frequency of methodological issues and reporting that would undermine the conclusions (Figure 3J).
Lack of appropriate background was the most common issue (179 cases), followed by FDR (94 cases), then lack of data shown (13), inference without test (11 cases), and misinterpreted FDR values (2 cases).
Only 35 analyses (15%) did not exhibit any of these major methodological issues.
Taken together, these data suggest methodological and reporting deficiencies are widespread in published functional enrichment analyses.

![Figure S1a. Representation of journals in the data set.](images/journals1.png "Journals")

![Figure S1B. Representation of different omics types in the data set.](images/omics1.png "Omics")

![Figure S1C. Representation of different organisms in the data set.](images/organisms1.png "Organisms")

![Figure 3A. Representation of different gene set libraries in the data set.](images/genesetlib1.png "Geneset libraries")

![Figure 3B. Proportion of analyses that reported version information for gene sets used.](images/genesetvers1.png "Geneset version")

![Figure 3C. Representation of different statistical tests in the data set.](images/stattest1.png "Geneset libraries")

![Figure 3D. Recognition of p-value correction for multiple testing in the data set.](images/fdr1.png "FDR")

![Figure 3E. Representation of different software tools for enrichment analysis in the data set.](images/app1.png "Software used")

![Figure 3F. Proportion of analyses that reported version information for software used.](images/appvers1.png "Software version")

![Figure 3G. Background gene set usage and reporting in the dataset.](images/bg1.png "Background gene list used")

![Figure 3H. Proportion of scripted analyses that provided software code.](images/code1.png "Code available")

![Figure 3I. Proportion of scripted analyses that provided gene profiles.](images/genelists1.png "Gene lists provided")

![Figure 3J. Proportion of analyses with different methodological problems.](images/assumptions1.png "Assumptions violated")

### Analysis scores

We then scored each analysis based on the inclusion or lack of methodological issues and included details.
The median score was -4, with a mean of -3.5 and standard deviation of 1.3 (Figure 4A).
Next, we assessed whether these analysis scores associate with journal citation metrics (Figure 4B).
There was a slight positive correlation using the Pearson method but it was not statistically significant (r=0.04, p=0.16).
Next we wanted to know which journals had the highest and lowest scores.
Only journals with five or more analyses were included.
The best scoring journals were *Transl Psychiatry*, *Metabolites* and *J Exp Clin Cancer Res* (Figure 4C), while the poorest were *Mol Med Rep*, *Cancer Med* and *World J Gastroenterol* (Figure 4D), although we note that there was a wide variation between articles of the same journal.
Then, we assessed for an association between mean analysis score and the SJR (Figure 4E).
Again, there was a slightly positive association that wasn't statistically significant (Pearson r=0.09, p=0.55).
Next we assessed whether there was any association between analysis scores and the number of citations received by the articles.
After log transforming the citation data, there was no association between citations and analysis scores (Pearson r=0.002, p=0.94).

![Figure 4A. Distribution of analysis scores.](images/hist1.png "Analysis scores")

![Figure 4B. Association of analysis scores with journal rank.](images/score_sjr1.png "Analysis scores versus journal rank")

![Figure 4C. Journals with highest analysis scores.](images/score_highest1.png "Highest analysis scores")

![Figure 4D. Journals with Lowest analysis scores.](images/score_lowest1.png "Lowest analysis scores")

![Figure 4E. Association between SJR and mean analysis score for journals with >=5 analyses.](images/mean_sjr1.png "Mean score vs SJR")

## Discussion

While there have been several articles evaluating the performance of functional enrichment tools, there has been relatively little interest in how existing tools are (mis)used. 
A recent study looked at the the popularity of frequently used tools and their performance (Xie et al, 2021), but this assumes that the tools were properly used.
In this sample of open access research articles, there appears to be a bias toward tools that are easy to use.
ORA tools that only require pasting lists of gene identifiers into a webpage (ie: DAVID, IPA, KOBAS and PANTHER) are collectively more popular than other solutions like GSEA (a standalone graphical user interface software for FCS) or any command line tool.
This is despite the fact that ORA tools perform worse than FCS and new generation pathway topology tools (Kaspi & Ziemann 2020; Maleki et al, 2020; Xie et al, 2021).

The lack of methodological details regarding enrichment analysis in some articles was disturbing.
To illustrate this, many articles mention prominently in the abstract the functional enrichment results but do not detail in their methods sections anything about what type of enrichment analysis was actually conducted.
This is not limited to a few obscure journals; there were 88 journals that published articles where the tool used for enrichment analysis was not defined.

As we screened these articles, we observed other types of details lacking.
For example GSEA has different options that impact the results including the ranking metric, gene weighting method and the permutation type (on samples or genes), which were not regularly reported in articles.
For ORA, in some instances it was unclear whether up- and down-regulated gene lists were combined before analysis.
When applying ORA to differential expression, combining up- and down-regulated gene lists is considered poor practice (Hong et al, 2014).
Our own tests indicate that combining up- and down-regulated gene lists prior to ORA results in a dramatic loss of sensitivity when the proportion of differentially expressed genes is high.
Using an inappropriate background list resulted in substantially altered results (Figure 1B).
In the example here, using the inappropriate background resulted in 2.3 fold more differentially regulated gene sets.
Thus if there are few or no significant results using a correct background list, there may be a temptation to use an inappropriate background.
Results here indicate most ORA articles do not report using a background gene list, suggesting that inappropriate ORA use is common.

We scored each of the analyses included in the screen which revealed only 30 articles scored a satisfactory score of zero or higher.
Surprisingly, the analysis scores we generated did not correlate with a journal level bibliographic metric, nor accrued citations of individual articles.
This suggests no association between methodological rigour and traditional research impact metrics in this sample.

Still there are some limitations that need to be recognised.
Many open access articles examined here are from journals not considered to be highly prestigious, and the distribution of analysis scores might be different for closed access articles.
The articles included in this study contained keywords related to functional enrichment in the abstract, and it is plausible that articles in more prestigious journals contain such details in the abstract at lower rates.

Nevertheless, these results are a wake up call for reproducibility and highlight the urgent need for minimum standards for functional enrichment analysis. 
To address this, we provide a set of minimum standards for functional enrichment analysis, as well as additional recommendations to achieve gold standard reliability and reproducibility (Box 1).

<hr>

### Essential minimum standards for functional enrichment analysis

The following guidelines are selected to avoid the most severe methodological issues while requiring minimal effort to address.

1. Report the origin of the gene sets and the version used or date downloaded.
These databases are regularly upgraded so this is important for reproducibility.

2. Report the tool used and its software version.
As these tools are regularly updated, this will aid reproducibility.

3. If making claims about the regulation of a gene set, then results of a statistical test must be shown (including measures of significance and effect size).
"Data not shown" is not acceptable when it is possible to attach supplementary files.

4. Report which statistical test was used.
This will help long term reproducibility, for example if in future the tool is no longer available.
This is especially relevant for tools that report the results of different statistical tests.

5. Always report FDR or q-values (p-values that have been corrected for multiple tests).
This will reduce the chance of false positives when performing many tests simultaneously.
Report what method was used for p-value adjustment; Bonferonni, FDR and FWER are examples.

6. If ORA is used, it is vital to use a background list consisting of all detected genes, and report this in the methods section.
Avoid tools that don't accept a background list.

7. Report selection criteria and parameters.
If performing ORA, then the inclusion thresholds need to be mentioned.
If using GSEA or another FCS tool, parameters around ranking, gene weighting and type of test need to be disclosed (eg: permuting sample labels or gene labels).
Any parameters that are vary from the defaults should be reported.

8. If using ORA for gene expression analysis, refrain from combining up- and down-regulated genes before analysis.

### Additional recommendations for enhanced reproducibility 

The following guidelines are directed at going over and above the minimum standards towards a gold standard of reliability and reproducibility.

9. Preference FCS and pathway topology tools over ORA tools.
ORA tools have a lower sensitivity. 
In practice, they are rarely used properly.

10. Include the gene profile data (including any background lists) in the supplementary data in TSV or CSV formats.
Excel spreadsheets are not recommended for genomics work (Ziemann et al, 2016)

11. Scripted analysis workflows are preferred over analysis involving graphical user interfaces because they can attain a higher degree of computational reproducibility, so consider investing in upgrading your analysis.
Similarly, free and open source software tools are preferred over proprietary software.

12. If using a scripted analysis, provide code by depositing it to a repository like GitHub or Zenodo.
Link the data to the code, so anyone can download and reproduce your analysis (Peng 2011).

<hr>

## Conclusions

We echo the sentiments of Timmons et al (2015) that functional enrichment analysis results should be interpreted with extreme caution, and that they shouldn't be considered proof of biological plausibility nor validity of omics data analysis.
Functional enrichment analysis is best used as a hypothesis generating procedure to inform subsequent biochemical or signaling experimentation.

## References

Draghici, S., Khatri, P., Martins, R. P., Ostermeier, G. C., & Krawetz, S. A. (2003). Global functional profiling of gene expression. Genomics, 81(2), 98–104.

Felisbino, M. B., Ziemann, M., Khurana, I., Okabe, J., Al-Hasani, K., Maxwell, S., … El-Osta, A. (2021). Valproic acid influences the expression of genes implicated with hyperglycaemia-induced complement and coagulation pathways. Scientific Reports, 11(1), 2163.

Hong, G., Zhang, W., Li, H., Shen, X., & Guo, Z. (2014). Separate enrichment analysis of pathways for up- and downregulated genes. Journal of the Royal Society, Interface, 11(92), 20130950.

Hosack, D. A., Dennis, G., Jr, Sherman, B. T., Lane, H. C., & Lempicki, R. A. (2003). Identifying biological themes within lists of genes with EASE. Genome Biology, 4(10), R70.

Huang, D. W., Sherman, B. T., & Lempicki, R. A. (2009). Bioinformatics enrichment tools: paths toward the comprehensive functional analysis of large gene lists. Nucleic Acids Research, 37(1), 1–13.

Jassal, B., Matthews, L., Viteri, G., Gong, C., Lorente, P., Fabregat, A., … D’Eustachio, P. (2020). The reactome pathway knowledgebase. Nucleic Acids Research, 48(D1), D498–D503.

Kaspi, A., & Ziemann, M. (2020). Mitch: Multi-contrast pathway enrichment for multi-omics and single-cell profiling data. BMC Genomics, 21(1), 447.

Love, M. I., Huber, W., & Anders, S. (2014). Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome Biology, 15(12), 550.

Maleki, F., Ovens, K., Hogan, D. J., & Kusalik, A. J. (2020). Gene set analysis: Challenges, opportunities, and future research. Frontiers in Genetics, 11, 654.

Peng, R. D. (2011). Reproducible research in computational science. Science (New York, N.Y.), 334(6060), 1226–1227.

Subramanian, A., Tamayo, P., Mootha, V. K., Mukherjee, S., Ebert, B. L., Gillette, M. A., … Mesirov, J. P. (2005). Gene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expression profiles. Proceedings of the National Academy of Sciences of the United States of America, 102(43), 15545–15550.

Timmons, J. A., Szkop, K. J., & Gallagher, I. J. (2015). Multiple sources of bias confound functional enrichment analysis of global -omics data. Genome Biology, 16(1), 186.

Xie, C., Jauhari, S., & Mora, A. (2021). Popularity and performance of bioinformatics software: the case of gene set analysis. BMC Bioinformatics, 22(1), 191.

Yu, G., Wang, L.-G., Han, Y., & He, Q.-Y. (2012). clusterProfiler: an R package for comparing biological themes among gene clusters. Omics: A Journal of Integrative Biology, 16(5), 284–287.

Ziemann, M., Eren, Y., & El-Osta, A. (2016). Gene name errors are widespread in the scientific literature. Genome Biology, 17(1), 177.

Ziemann, M., Kaspi, A., & El-Osta, A. (2019). Digital expression explorer 2: a repository of uniformly processed RNA sequencing data. GigaScience, 8(4). doi:10.1093/gigascience/giz022